{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2fa251",
   "metadata": {},
   "source": [
    "# {{ title }}\n",
    "\n",
    "- **Student:** {{ authorName }}\n",
    "- **Year:** {{ year}}\n",
    "\n",
    "{{ projectDescription }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb5390",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "In this section, we'll ensure your environment is ready to build an AI Agent. We'll install the required Python packages and verify your Python and Jupyter versions.\n",
    "\n",
    "**Requirements:**\n",
    "{{ requirementsList }}\n",
    "\n",
    "Let's install the required Python packages and check your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba93349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.install_and_import import install_and_import\n",
    "from utils.load_file import load_file\n",
    "from utils.print_setup_status import print_setup_status\n",
    "\n",
    "required = load_file('./requirements.txt').strip().splitlines()\n",
    "for pkg in required:\n",
    "    install_and_import(pkg)\n",
    "\n",
    "print_setup_status(pkg_list=required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf45bd9",
   "metadata": {},
   "source": [
    "## Initializing Ollama in Notebook\n",
    "Now, let's connect to your local (or remote) Ollama LLM instance. We'll send a simple prompt and display the response.\n",
    "\n",
    "**Instructions:**\n",
    "- Make sure the Ollama server is running locally (`ollama serve`) or update the API URL if remote.\n",
    "- We'll use the `requests` library to interact with the Ollama API.\n",
    "\n",
    "**References:**\n",
    "- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ollama_chat import ollama_chat\n",
    "\n",
    "result = ollama_chat()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c205c1",
   "metadata": {},
   "source": [
    "## Integrating pandas for Data Workflows\n",
    "We'll now use `pandas` to load and preprocess a knowledge base (CSV or DataFrame). This data can be used to provide context to the agent.\n",
    "\n",
    "**Steps:**\n",
    "- Load a CSV file or create a DataFrame with sample knowledge.\n",
    "- Preprocess text (e.g., lowercasing, removing punctuation).\n",
    "- Prepare the data for use as context in LLM prompts.\n",
    "\n",
    "**Tip:**\n",
    "- You can replace the sample data with your own CSV or DataFrame as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51744d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config.data import SAMPLE_DATA\n",
    "from utils.preprocess_text import preprocess_text\n",
    " \n",
    "data = SAMPLE_DATA\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "df[\"question_clean\"] = df[\"question\"].apply(preprocess_text)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec5a34",
   "metadata": {},
   "source": [
    "## Agent Design Overview\n",
    "\n",
    "{{ agentDesignOverview }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0369308",
   "metadata": {},
   "source": [
    "## Interacting with the AI Agent\n",
    "Let's create a simple command-line interface (CLI) for our agent using `rich`. We'll use a command loop to interact with the agent, and `rich.Console` to print stylized prompts, responses, and tables. Let's launch an interactive session! This cell combines everything:\n",
    "\n",
    "**Instructions:**\n",
    "- Type a question. If it's in the knowledge base, you'll get an instant answer.\n",
    "- Otherwise, the agent will ask Ollama.\n",
    "- Type `exit` or `quit` to end the session.\n",
    "\n",
    "**Tip:**\n",
    "- This CLI will run in the notebook output cell, but you can adapt it for a terminal script as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from utils.format_knowledge_base import format_knowledge_base\n",
    "from utils.agent_cli import agent_cli\n",
    "\n",
    "console = Console()\n",
    "\n",
    "try:\n",
    "    table = format_knowledge_base(df)\n",
    "    console.print(table)\n",
    "    agent_cli(console=console, df=df)\n",
    "except KeyboardInterrupt:\n",
    "    console.print(\"\\n[bold yellow]AI Agent interrupted. Goodbye![/bold yellow]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[bold red]Unexpected error: {e}[/bold red]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
